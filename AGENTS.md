Development Brief: Morpheus' Mirror
Document Version: 1.0
Date: July 29, 2025
Project Lead: Gemini & You

1. Elevator Pitch
An interactive, voice-led dream journal that uses a sophisticated AI analyst to discuss your dreams, and then transforms your own sketches of dream scenes into stunning, artistic images, creating a truly multimodal and personal exploration of the subconscious.

2. Core Concept & Vision
The project aims to create an experience that goes beyond simple transcription or Q&A. It positions the AI as a partner in discovery. The user doesn't just tell the app about their dream; they explore it with a guide.

The core interaction is a spoken conversation with an AI persona modeled on a Jungian analyst. This AI asks insightful, probing questions to help the user unravel their own symbolism. The "wow" moment comes when the user is invited to sketch a key element of their dream. The application then uses multimodal AI to interpret the sketch in the context of the conversation, generating a rich, artistic image that reflects both the drawing's form and the dream's emotional content.

3. Key Features & Scope (MVP for Sprint)
Voice-to-Voice Conversation: Seamless interaction using browser-based Speech-to-Text (STT) and Text-to-Speech (TTS).

Context-Aware AI Analyst: The AI will maintain conversation history to provide relevant, insightful follow-up questions, adopting a consistent Jungian persona.

Sketch-to-Image Generation: A simple drawing canvas for the user to sketch on.

Multimodal Interpretation: The system will analyze both the sketch and the preceding conversation to generate a deeply relevant image.

Integrated UI: A clean, single-page interface that presents the conversation, the drawing canvas, and the final generated image.

4. Technical Architecture & Stack
Frontend:

Framework: Vanilla HTML, CSS, and JavaScript. (No complex frameworks needed for the MVP).

Voice I/O: Browser's native Web Speech API (SpeechRecognition for input, speechSynthesis for output).

Drawing: A lightweight JavaScript library like drawingboard.js or native HTML5 Canvas API for the sketchpad.

Backend:

Framework: Python with Flask (or FastAPI). Simple, fast to set up, and excellent for API endpoints.

Environment: python-dotenv for managing the API key securely.

Google AI & Gemini API Usage:

Conversational Agent: gemini-pro for its strong language, reasoning, and multi-turn chat capabilities.

Image Prompt Generation: gemini-1.5-pro for its multimodal capabilities. This is the key to the magic: it will take the sketch (image) and the conversation (text) as input to generate a new, highly-detailed artistic prompt.

Image Generation: Imagen (or a similar high-quality image generation model) will be called using the prompt generated by gemini-1.5-pro.

5. User & Data Flow (The Core Logic)
This is the critical sequence of events:

Conversation Loop:

User: Speaks to the browser.

Frontend (JS): Captures audio via Web Speech API, converts to text.

Frontend (JS): POSTs the user's text and conversation history to the backend /chat endpoint.

Backend (Python): Receives the request, appends the new message to the history, and sends it all to the gemini-pro API with the Jungian system prompt.

Backend (Python): Receives the text response from Gemini and returns it to the frontend.

Frontend (JS): Renders the AI's text response in the chat log and speaks it aloud using speechSynthesis.

Image Generation Flow:

User: Clicks "Draw," sketches on the canvas, and clicks "Generate Vision."

Frontend (JS): Exports the canvas drawing as a Base64 encoded image string.

Frontend (JS): POSTs the image string and the entire conversation history to the backend /generate-image endpoint.

Backend (Python): Receives the request. It calls gemini-1.5-pro, providing it with:

The "Art Director" system prompt.

The full conversation history (text).

The user's sketch (image).

Backend (Python): gemini-1.5-pro returns a rich, detailed text prompt for image generation.

Backend (Python): It then takes this new text prompt and calls the Imagen API.

Backend (Python): Imagen returns the final image. The backend sends the image URL back to the frontend.

Frontend (JS): Receives the image URL and displays the final, stunning artwork to the user.

6. Sprint Plan & Milestones
Milestone 1 (The Backbone): Set up the Flask backend with a /chat endpoint that successfully communicates with gemini-pro. Test using a tool like Postman or curl. Set up the basic HTML/CSS frontend.

Milestone 2 (The Conversation): Implement the text-to-speech and speech-to-text functionality in the frontend. Achieve a full, voice-led conversation loop.

Milestone 3 (The Canvas): Integrate the drawing canvas into the UI. Ensure the frontend can capture the sketch as a Base64 string.

Milestone 4 (The Magic): Build the /generate-image endpoint. Implement the full multimodal logic: sending the sketch and text to gemini-1.5-pro, getting the new prompt, and using that to generate the final image. Display the image in the UI.

7. Demo "Wow" Factor
The demo pitch is simple: "I had a dream about a lonely house on a hill. Let me tell my analyst about it..." After a brief conversation, "...and let me show you what it looked like." A rough sketch is drawn. A button is clicked. A few seconds later, a beautiful, evocative piece of art appears on the screen, perfectly capturing the mood of the conversation. The final killer blow is when the AI analyst seamlessly continues the conversation, commenting on the newly created image.
